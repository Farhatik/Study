{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erc2A41DMzku"
   },
   "source": [
    "# Введение в глубинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Практическое задание 2. Рекуррентные Нейронные Сети\n",
    "\n",
    "## Общая информация\n",
    "Дата выдачи: 04.02.2021\n",
    "\n",
    "Мягкий дедлайн: 19.02.2021 00:59 MSK\n",
    "\n",
    "Жёсткий дедлайн: 22.02.2021 00:59 MSK\n",
    "\n",
    "## Оценивание и штрафы\n",
    "\n",
    "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимально допустимая оценка за работу — 10 баллов.\n",
    "\n",
    "Сдавать задание после указанного срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "\n",
    "## Формат сдачи\n",
    "Задания сдаются семинаристу на почту. Посылка должна содержать:\n",
    "* Ноутбук homework-practice-02-Username.ipynb\n",
    "\n",
    "Username — ваша фамилия на латинице"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqWw3aP7wgOL"
   },
   "source": [
    "## 0. Подготовка данных (0 баллов)\n",
    "\n",
    "Данные представляют собой корпус текстов с 4-мя категориями. Ваша задача - написать классификатор для этих данных, определяющий, к какой из категорий относится текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2cTXscGF5waG"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PUs6MXNA5waI"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', engine=None)\n",
    "df_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx2PZCCkQu91"
   },
   "source": [
    "## 1. Предобработка данных (2 балла)\n",
    "\n",
    "В этом задании вам предстоит провести предобработку данных. Баллы ставятся следующим образом:\n",
    "\n",
    "* Привести все тексты к одной длине, заменить слова/токены на числа, факторизовать целевую переменную и т.д. (1 балл)\n",
    "\n",
    "* Использовать токенизатор, который разбил бы все слова на токены (подробнее https://github.com/huggingface/tokenizers). (1 балл)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7J8p9YJf5waI"
   },
   "source": [
    "### Разведовательный анализ данных\n",
    "Исследуем наш target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "2vqSFvLD5waI",
    "outputId": "31871dcc-b720-4f7c-b830-488593109c4c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mchsgov</th>\n",
       "      <td>2570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mil</th>\n",
       "      <td>2562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mospolice</th>\n",
       "      <td>2427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>russianpost</th>\n",
       "      <td>2235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             text\n",
       "source           \n",
       "mchsgov      2570\n",
       "mil          2562\n",
       "mospolice    2427\n",
       "russianpost  2235"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('source').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB0DNsr4aduK"
   },
   "source": [
    "Факторизуем целевую переменную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RmQ4PQF05waJ"
   },
   "outputs": [],
   "source": [
    "factorize_dic = {'mchsgov' : 0, 'mil' : 1, 'mospolice' : 2, 'russianpost' : 3}\n",
    "unfactorize_dic = {0 : 'mchsgov', 1 : 'mil', 2 : 'mospolice', 3 : 'russianpost'}\n",
    "\n",
    "def replace_target(df, dic):\n",
    "  df = df.fillna('')\n",
    "  df['source'] = df_train['source'].replace(dic)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sIjRHAPlcl0R"
   },
   "outputs": [],
   "source": [
    "def preprocess_target(df, dic): \n",
    "  df = replace_target(df, dic)\n",
    "  return pd.get_dummies(df['source']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZMxLaO8kDd0Q"
   },
   "outputs": [],
   "source": [
    "#https://github.com/susanli2016/NLP-with-Python/blob/master/Multi-Class%20Text%20Classification%20LSTM%20Consumer%20complaints.ipynb\n",
    "#https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "#Спасибо Susan Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBc32ixn5waJ",
    "outputId": "ca41fc5f-b9af-47f7-979b-29ddf4a849e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "exclude = set(string.punctuation)\n",
    "exclude.add(\"«\")\n",
    "exclude.add(\"»\")\n",
    "exclude.add(\"b\")\n",
    "exclude.add(\"r\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('russian'))\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^а-я #+_]')\n",
    "\n",
    "def preproces_text(text):\n",
    "    text = text.lower()\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    text = ''.join(ch for ch in text if ch not in exclude)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vMiZtHAwbg6P"
   },
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 2000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "def tokine_data(df):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "    tokenizer.fit_on_texts(df['text'].values)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "S43MJbWDbPAl"
   },
   "outputs": [],
   "source": [
    "def preproces_data(df):\n",
    "    df = df.fillna(\"\")\n",
    "    df['text'] = df['text'].apply(preproces_text)\n",
    "    tokenizer = tokine_data(df)\n",
    "    X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-XmxjpiwgOV"
   },
   "source": [
    "## 2. LSTM-сеть (4 балла)\n",
    "]\n",
    "В этом задании вам предстоит написать LSTM сеть __вручную__ (то есть без использования стандартных реализаций из keras / torch / tensorflow). Сама архитектура отлично расписана здесь: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r0a81YTocWlX"
   },
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gjeg3rwEHCJp"
   },
   "source": [
    "## 3. Модель (2 балла)\n",
    "\n",
    "В этом задании вам предстоит объединить вашу сеть с несколькими другими слоями для создания итоговой модели классификатора (можно начать с самой базовой архитектуры, слой эмбеддингов - LSTM - выходной слой)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MW0-PlMy5waO",
    "outputId": "121b9867-0996-46a9-855e-4bf6c176a7ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "<bound method Model.summary of <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fd903e4a4d0>>\n"
     ]
    }
   ],
   "source": [
    "#https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "#Спасибо Susan Li\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "uy4eL7fDIfmY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24VNfcMWHaKJ"
   },
   "source": [
    "## 4. Обучение модели (4 балла)\n",
    "\n",
    "**Важно!** Public Leaderboard содержит только 30% тестовых данных, баллы за задание будут измеряться в соответствии со всеми тестовыми данными, так что после завершения соревнования Ваша позиция на leaderboard-е может поменяться.\n",
    "\n",
    "**Важно!** При использовании **СВОЕЙ** модели LSTM полученные баллы удваиваются.\n",
    "\n",
    "* val_accuracy > baseline: 1 балл\n",
    "\n",
    "* Ваша позиция на лидерборде между 25% (включительно) и 50% (невключительно) квантилями среди студентов, перебивших baseline: 2 балла\n",
    "\n",
    "* Ваша позиция на лидерборде между 50% и 75% квантилями среди студентов, перебивших baseline: 3 балла\n",
    "\n",
    "* Ваша позиция на лидерборде между 75% и 90% квантилями среди студентов, перебивших baseline: 4 балла\n",
    "\n",
    "* Ваша позиция на лидерборде больше 90% квантиля среди студентов, перебивших baseline: 5 баллов\n",
    "\n",
    "* Вы в топ-3 студентов на лидерборде: 10 баллов\n",
    "\n",
    "\n",
    "**Максимальный суммарный балл** - 15\n",
    "\n",
    "**Финальная оценка**: min(суммарный балл, 10), доп баллы можно использовать для увеличения оценок за самостоятельные работы (1 доп балл = +1 балл к самостоятельной)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdyWT5OHCSr3",
    "outputId": "bc93ae76-d0b7-4e7a-b6ef-1b9dfb2b8758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76190 unique tokens.\n",
      "Epoch 1/3\n",
      "78/78 [==============================] - 72s 879ms/step - loss: 1.0690 - accuracy: 0.5650\n",
      "Epoch 2/3\n",
      "78/78 [==============================] - 67s 864ms/step - loss: 0.2021 - accuracy: 0.9468\n",
      "Epoch 3/3\n",
      "78/78 [==============================] - 69s 880ms/step - loss: 0.1472 - accuracy: 0.9602\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "history = model.fit(preproces_data(df_train), preprocess_target(df_train, factorize_dic) , \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGldC1sIhg81",
    "outputId": "0d6b8eab-7ee9-43a4-db8c-5ca8e845987f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29057 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "answer_df = pd.DataFrame(np.argmax(model.predict(preproces_data(df_test)), axis=1)).replace(unfactorize_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "OokB1xwsmSr_"
   },
   "outputs": [],
   "source": [
    "\n",
    "answer_df = (answer_df.reset_index()).rename(columns={'index' : 'Id', 0 : 'Category'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "grzSk_1flp1n"
   },
   "outputs": [],
   "source": [
    "answer_df.to_csv('answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKkqWP90Tswn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
